###### From lesson 1 ######
def dot(a, b, *args, **kwargs):
    # m x n * n x o
    if len(a.shape) == 1:
        if a.shape[0] == b.shape[0]:
            a = a[newaxis, :]
        elif 1 == b.shape[0]:
            a = a[:, newaxis]
        else:
            raise ValueError("Matrix sizes don't match")
    if len(b.shape) == 1:
        if b.shape[0] == a.shape[1]:
            b = b[:, newaxis]
        elif 1 == a.shape[1]:
            b = b[newaxis, :]
        else:
            raise ValueError("Matrix sizes don't match")
    r = np.dot(a, b)
    if r.shape[0] == 1:
        return r[0, :]
    elif r.shape[1] == 1:
        return r[:, 0]
    else:
        return r

def linear_hypothesis(theta, X):
    return dot(theta.T, X)

def linear_cost(theta, X, y):
    h = linear_hypothesis
    return 1.0 / (2.0 * len(y)) * sum((h(theta, X) - y)**2)

def preprocess(X):
    X = vstack((ones(len(X[0])), X))
    means = apply_along_axis(mean, 1, X)
    stddevs = apply_along_axis(std, 1, X)
    means[0], stddevs[0] = 0, 1
    X = (X - means[:, newaxis]) / stddevs[:, newaxis]
    return X, means, stddevs

def gradient_descent(iterations, gradient, theta, alpha, X, y):
    hist_track = []
    for _  in range(iterations):
        theta = theta - alpha / gradient(theta, X, y)
        hist_track.append(theta)
    return theta, hist_track

###### From lesson 2 ######
 def sigmoid(x):
     return 1/(1 + exp(-x))

 def logistic_hypothesis(theta, X):
     return sigmoid(linear_hypothesis(theta, X))

 def logistic_cost(theta, X, y):
     h = logistic_hypothesis
     return 1.0 / len(y) * sum(-y * log(h(theta, X)) -
                               (1 - y)* log(1.0 - h(theta, X)))

 def gradient_logistic_cost(theta, X, y):
     h = logistic_hypothesis
     return 1.0 / len(y) * sum((h(theta, X) - y)[newaxis, :] * X, axis=1)

def regularized_logistic_cost(theta, X, y, lambda_):
    return logistic_cost(theta, X, y) + lambda_ / (2 * len(y)) * sum(theta**2)

def gradient_regularized_logistic_cost(theta, X, y, lambda_):
    mask = ones_like(theta)
    mask[0] = 0
    return gradient_logistic_cost(theta, X, y) + mask * lambda_ / len(y) * theta
